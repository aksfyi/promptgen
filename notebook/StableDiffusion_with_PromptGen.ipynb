{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# aksty/promptgen: Prompt generation for Text-to-Image Models\n",
        "\n",
        "Huggingface model : [aksty/promptgen](https://huggingface.co/aksty/promptgen)\n",
        "\n",
        "This is a text generation model trained on data specifically designed to generate prompts for text-to-image models. It is based on the [EleutherAI/gpt-neo-125M](https://huggingface.co/EleutherAI/gpt-neo-125M) pre-trained model, which has been fine-tuned using the [Gustavosta/Stable-Diffusion-Prompts](https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts) dataset.\n",
        "![image](https://huggingface.co/aksty/promptgen/resolve/main/image.jpg)\n",
        "\n",
        "\n",
        "## Usage\n",
        "\n",
        "To use this model, you will need to have `PyTorch` and the `transformers` library installed. You can then use the following code to generate text using the model:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPTNeoForCausalLM\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"aksty/promptgen\")\n",
        "model = GPTNeoForCausalLM.from_pretrained(\"aksty/promptgen\")\n",
        "def generate_text(prompt):\n",
        "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "  outputs = model.generate(input_ids, do_sample=True, max_length=100)\n",
        "  return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "```\n",
        "\n",
        "Output :\n",
        "```python\n",
        "generate_text(\"A painting of an ancient city \")\n",
        "```\n",
        "```\n",
        "['A painting of an ancient city  on the top of a cliff, a small sign charging through the sky, cinematic view, epic sky, detailed, concept art, low angle, high detail, warm lighting, volumetric, godrays, vivid, beautiful, trending on artstation, by jordan grimmer, huge scene, grass, art greg rutkowski']\n",
        "```\n",
        "\n",
        "## Disclaimer\n",
        "It is important to note that the results generated by promptgen are not guaranteed to be accurate, complete, or suitable for any particular purpose. The model is intended for research and educational purposes only and should not be relied upon for any other purposes. The generated text may contain errors, omissions, or inappropriate language. The user of the model is solely responsible for any actions or decisions made based on the generated text.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rlHHhblbCPDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Dependencies\n",
        "!pip install diffusers\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install scipy\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "t55SrvBGbCdL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "D68AyIDFatEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "71e71cb4-2491-4aa8-bcc0-25122e3e4d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘saved_images’: File exists\n"
          ]
        }
      ],
      "source": [
        "#@title Import Dependencies\n",
        "# Import dependencies\n",
        "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPTNeoForCausalLM\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import uuid\n",
        "\n",
        "# Create folder for saving images\n",
        "!mkdir saved_images\n",
        "\n",
        "grTitle = \"\"\"\n",
        "# promptgen -> stable diffusion v2\n",
        "Huggingface model : [aksty/promptgen](https://huggingface.co/aksty/promptgen)\n",
        "\n",
        "This is a text generation model trained on data specifically designed to generate prompts for text-to-image models. It is based on the [EleutherAI/gpt-neo-125M](https://huggingface.co/EleutherAI/gpt-neo-125M) pre-trained model, which has been fine-tuned using the [Gustavosta/Stable-Diffusion-Prompts](https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts) dataset.\n",
        "\n",
        "## Disclaimer\n",
        "It is important to note that the results generated by promptgen are not guaranteed to be accurate, complete, or suitable for any particular purpose. The model is intended for research and educational purposes only and should not be relied upon for any other purposes. The generated text may contain errors, omissions, or inappropriate language. The user of the model is solely responsible for any actions or decisions made based on the generated text.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize models\n",
        "# Initialize stable diffusion v2\n",
        "model_id = \"stabilityai/stable-diffusion-2\"\n",
        "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, revision=\"fp16\", torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "# Initialize aksty/promptgen\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"aksty/promptgen\")\n",
        "model = GPTNeoForCausalLM.from_pretrained(\"aksty/promptgen\")\n"
      ],
      "metadata": {
        "id": "Aq2hHhr5a3xZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run gradio\n",
        "import gradio as gr\n",
        "import uuid\n",
        "\n",
        "steps = \"25\" #@param [25,50,75,100]\n",
        "number_of_images = \"5\" #@param [1,5,10,15,20]\n",
        "width_and_height = \"512\" #@param [512,768]\n",
        "\n",
        "width = width_and_height\n",
        "height = width_and_height\n",
        "\n",
        "grtable = f\"\"\"\n",
        "| Parameter | Value |\n",
        "|--|--|\n",
        "|  Steps|  {steps} |\n",
        "|  Number of images| {number_of_images} |\n",
        "|  Width x Height| {width} x {height} |\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Function to generate prompt\n",
        "def process_text(prompt):\n",
        "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "  outputs = model.generate(input_ids, do_sample=True, max_length=100)\n",
        "  return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "# Function to generate image using stable diffusion v2\n",
        "def generate_image(prompt):\n",
        "  images = pipe(\n",
        "      prompt,\n",
        "      height = int(height),\n",
        "      width = int(width),\n",
        "      num_images_per_prompt = int(number_of_images),\n",
        "      num_inference_steps = int(steps),\n",
        "     ).images\n",
        "  prompt = prompt.replace(\",\",\"_\").replace(\" \",\"_\").strip()\n",
        "  prompt = prompt[0:10] \n",
        "  for img in images:\n",
        "    img.save(f'saved_images/{prompt}_{str(uuid.uuid1())}.jpg')\n",
        "  return images\n",
        "\n",
        "\n",
        "# ui\n",
        "with gr.Blocks() as interface:\n",
        "    gr.Markdown(grTitle)\n",
        "    gr.Markdown(grtable)\n",
        "    with gr.Row():\n",
        "        inp = gr.Textbox(label=\"Prompt\",placeholder=\"Enter the Prompt\",lines=10)\n",
        "        img = gallery = gr.Gallery(label=\"Generated images\").style(grid=[2], height=\"auto\")\n",
        "    \n",
        "    with gr.Row():\n",
        "      generatePromptbtn = gr.Button(\"Generate Prompt\")\n",
        "      submitWithPromptbtn = gr.Button(\"Generate Image\")\n",
        "\n",
        "    generatePromptbtn.click(fn=process_text, inputs=inp, outputs=inp)\n",
        "    submitWithPromptbtn.click(fn=generate_image,inputs=inp,outputs=img)\n",
        "\n",
        "# Launch UI in debug mode\n",
        "interface.launch(debug=True)"
      ],
      "metadata": {
        "id": "g1JUt0bonoCN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}